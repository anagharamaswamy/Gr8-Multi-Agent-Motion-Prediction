{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl3idnY4Mqhj"
      },
      "outputs": [],
      "source": [
        "\\pip install l5kit\n",
        "!pip install -U PyYAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgp6vqdlMt7l"
      },
      "outputs": [],
      "source": [
        "!pip uninstall opencv-python-headless==4.5.5.62\n",
        "!pip install opencv-python-headless==4.1.2.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9qtO3eaMuAf"
      },
      "outputs": [],
      "source": [
        "%cd lyftdataset\n",
        "!unzip -q lyft-motion-prediction-autonomous-vehicles.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unEbwu_7Mt9-"
      },
      "outputs": [],
      "source": [
        "!mkdir /root/.kaggle/\n",
        "!touch /root/.kaggle/kaggle.json\n",
        "!echo '{\"username\":\"simarkareer\",\"key\":\"ff816530aeb5eda4d7ce160a471cbe14\"}' >> /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json\n",
        "!mkdir lyftdataset\n",
        "!cd lyftdataset\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle --version\n",
        "!kaggle competitions download -c lyft-motion-prediction-autonomous-vehicles -p /content/lyftdataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wcvhWFdP4MW"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/lyft/l5kit/master/examples/agent_motion_prediction/agent_motion_config.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUn8BkOCP95n"
      },
      "outputs": [],
      "source": [
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ne8_95ZP_a_"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50\n",
        "from tqdm import tqdm\n",
        "\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QX7C7ExQQru"
      },
      "outputs": [],
      "source": [
        "class EgoNeighborDataset(EgoDataset):\n",
        "    def get_frame(self, scene_index: int, state_index: int, track_id: Optional[int] = None) -> dict:\n",
        "        \"\"\"\n",
        "        A utility function to get the rasterisation and trajectory target for a given agent in a given frame\n",
        "        Args:\n",
        "            scene_index (int): the index of the scene in the zarr\n",
        "            state_index (int): a relative frame index in the scene\n",
        "            track_id (Optional[int]): the agent to rasterize or None for the AV\n",
        "        Returns:\n",
        "            dict: the rasterised image in (Cx0x1) if the rast is not None, the target trajectory\n",
        "            (position and yaw) along with their availability, the 2D matrix to center that agent,\n",
        "            the agent track (-1 if ego) and the timestamp\n",
        "        \"\"\"\n",
        "        frames = self.dataset.frames[get_frames_slice_from_scenes(self.dataset.scenes[scene_index])]\n",
        "\n",
        "        tl_faces = self.dataset.tl_faces\n",
        "        try:\n",
        "            if self.cfg[\"raster_params\"][\"disable_traffic_light_faces\"]:\n",
        "                tl_faces = np.empty(0, dtype=self.dataset.tl_faces.dtype)  # completely disable traffic light faces\n",
        "        except KeyError:\n",
        "            warnings.warn(\n",
        "                \"disable_traffic_light_faces not found in config, this will raise an error in the future\",\n",
        "                RuntimeWarning,\n",
        "                stacklevel=2,\n",
        "            )\n",
        "        data = self.sample_function(state_index, frames, self.dataset.agents, tl_faces, track_id)\n",
        "\n",
        "        # add information only, so that all data keys are always preserved\n",
        "        data[\"host_id\"] = self.dataset.scenes[scene_index][\"host\"]\n",
        "        data[\"timestamp\"] = frames[state_index][\"timestamp\"]\n",
        "        data[\"track_id\"] = np.int64(-1 if track_id is None else track_id)  # always a number to avoid crashing torch\n",
        "        data[\"world_to_image\"] = data[\"raster_from_world\"]  # TODO deprecate\n",
        "        data[\"\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if data[\"image\"] is not None:\n",
        "            data[\"image\"] = data[\"image\"].transpose(2, 0, 1)  # 0,1,C -> C,0,1\n",
        "        else:\n",
        "            del data[\"image\"]\n",
        "\n",
        "        return data\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqKdPpnKQvRj"
      },
      "outputs": [],
      "source": [
        "# set env variable for data\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/content/lyftdataset\"\n",
        "dm = LocalDataManager(None)\n",
        "# get config\n",
        "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
        "print(cfg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIATYB8SQ3nD"
      },
      "outputs": [],
      "source": [
        "def build_model(cfg: Dict) -> torch.nn.Module:\n",
        "    # load pre-trained Conv2D model\n",
        "    model = resnet50(pretrained=True)\n",
        "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "    num_in_channels = 3 + num_history_channels\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        num_in_channels,\n",
        "        model.conv1.out_channels,\n",
        "        kernel_size=model.conv1.kernel_size,\n",
        "        stride=model.conv1.stride,\n",
        "        padding=model.conv1.padding,\n",
        "        bias=False,\n",
        "    )\n",
        "    # change output size to (X, Y) * number of future states\n",
        "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
        "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeathgUQQ3zz"
      },
      "outputs": [],
      "source": [
        "def forward(data, model, device, criterion):\n",
        "    inputs = data[\"image\"].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    print(\"\")\n",
        "    print(\"-\"*100)\n",
        "    print(data[\"target_positions\"].shape)\n",
        "    print(data[\"target_positions\"])\n",
        "    print(data[\"history_positions\"].shape)\n",
        "    print(data[\"history_positions\"])\n",
        "    print(data[\"track_id\"])\n",
        "    print(\"-\"*100)\n",
        "    targets = data[\"target_positions\"].to(device)\n",
        "    # Forward pass\n",
        "    outputs = model(inputs).reshape(targets.shape)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
        "    loss = loss * target_availabilities\n",
        "    loss = loss.mean()\n",
        "    return loss, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I08NFHRCQ32O"
      },
      "outputs": [],
      "source": [
        "# ===== INIT DATASET\n",
        "train_cfg = cfg[\"train_data_loader\"]\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
        "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
        "                             num_workers=train_cfg[\"num_workers\"])\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jR1fjhFQ345"
      },
      "outputs": [],
      "source": [
        "# ==== INIT MODEL\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = build_model(cfg).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss(reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCTlDMomQ37A"
      },
      "outputs": [],
      "source": [
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "for _ in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    loss, _ = forward(data, model, device, criterion)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses_train.append(loss.item())\n",
        "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9612shKwssG5"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO4JAx0ussPZ"
      },
      "outputs": [],
      "source": [
        "# ===== GENERATE AND LOAD CHOPPED DATASET\n",
        "num_frames_to_chop = 100\n",
        "eval_cfg = cfg[\"val_data_loader\"]\n",
        "eval_base_path = create_chopped_dataset(dm.require(eval_cfg[\"key\"]), cfg[\"raster_params\"][\"filter_agents_threshold\"], \n",
        "                              num_frames_to_chop, cfg[\"model_params\"][\"future_num_frames\"], MIN_FUTURE_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rre-8CrYssSm"
      },
      "outputs": [],
      "source": [
        "eval_zarr_path = str(Path(eval_base_path) / Path(dm.require(eval_cfg[\"key\"])).name)\n",
        "eval_mask_path = str(Path(eval_base_path) / \"mask.npz\")\n",
        "eval_gt_path = str(Path(eval_base_path) / \"gt.csv\")\n",
        "\n",
        "eval_zarr = ChunkedDataset(eval_zarr_path).open()\n",
        "eval_mask = np.load(eval_mask_path)[\"arr_0\"]\n",
        "# ===== INIT DATASET AND LOAD MASK\n",
        "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer, agents_mask=eval_mask)\n",
        "eval_dataloader = DataLoader(eval_dataset, shuffle=eval_cfg[\"shuffle\"], batch_size=eval_cfg[\"batch_size\"], \n",
        "                             num_workers=eval_cfg[\"num_workers\"])\n",
        "print(eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn13K6FwssUs"
      },
      "outputs": [],
      "source": [
        "# ==== EVAL LOOP\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# store information for evaluation\n",
        "future_coords_offsets_pd = []\n",
        "timestamps = []\n",
        "agent_ids = []\n",
        "\n",
        "progress_bar = tqdm(eval_dataloader)\n",
        "for data in progress_bar:\n",
        "    _, ouputs = forward(data, model, device, criterion)\n",
        "    \n",
        "    # convert agent coordinates into world offsets\n",
        "    agents_coords = ouputs.cpu().numpy()\n",
        "    world_from_agents = data[\"world_from_agent\"].numpy()\n",
        "    centroids = data[\"centroid\"].numpy()\n",
        "    coords_offset = []\n",
        "    \n",
        "    for agent_coords, world_from_agent, centroid in zip(agents_coords, world_from_agents, centroids):\n",
        "        coords_offset.append(transform_points(agent_coords, world_from_agent) - centroid[:2])\n",
        "    \n",
        "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
        "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
        "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9oz0BzlsxWv"
      },
      "outputs": [],
      "source": [
        "pred_path = f\"{gettempdir()}/pred.csv\"\n",
        "\n",
        "write_pred_csv(pred_path,\n",
        "               timestamps=np.concatenate(timestamps),\n",
        "               track_ids=np.concatenate(agent_ids),\n",
        "               coords=np.concatenate(future_coords_offsets_pd),\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHZ8IMgVsxZV"
      },
      "outputs": [],
      "source": [
        "print(pred_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxzJF6VCsxb-"
      },
      "outputs": [],
      "source": [
        "metrics = compute_metrics_csv(eval_gt_path, pred_path, [neg_multi_log_likelihood, time_displace])\n",
        "for metric_name, metric_mean in metrics.items():\n",
        "    print(metric_name, metric_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h80ooPrKs4O6"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# build a dict to retrieve future trajectories from GT\n",
        "gt_rows = {}\n",
        "for row in read_gt_csv(eval_gt_path):\n",
        "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
        "\n",
        "eval_ego_dataset = EgoDataset(cfg, eval_dataset.dataset, rasterizer)\n",
        "\n",
        "i = 0\n",
        "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
        "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
        "    if not len(agent_indices):\n",
        "        continue\n",
        "\n",
        "    # get AV point-of-view frame\n",
        "    data_ego = eval_ego_dataset[frame_number]\n",
        "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
        "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
        "    \n",
        "    predicted_positions = []\n",
        "    target_positions = []\n",
        "\n",
        "    for v_index in agent_indices:\n",
        "        data_agent = eval_dataset[v_index]\n",
        "\n",
        "        out_net = model(torch.from_numpy(data_agent[\"image\"]).unsqueeze(0).to(device))\n",
        "        out_pos = out_net[0].reshape(-1, 2).detach().cpu().numpy()\n",
        "        # store absolute world coordinates\n",
        "        predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
        "        # retrieve target positions from the GT and store as absolute coordinates\n",
        "        track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
        "        target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
        "\n",
        "\n",
        "    # convert coordinates to AV point-of-view so we can draw them\n",
        "    predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
        "    target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
        "\n",
        "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
        "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
        "    i += 1\n",
        "    plt.imshow(im_ego[::-1])\n",
        "    plt.savefig(\"ResNeXt_\"+str(i)+\".png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnEU3Ehls4TM"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, clear_output\n",
        "import PIL\n",
        " \n",
        "dm = LocalDataManager()\n",
        "dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\n",
        "zarr_dataset = ChunkedDataset(dataset_path)\n",
        "zarr_dataset.open()\n",
        "# print(zarr_dataset)\n",
        "\n",
        " \n",
        "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
        "rast = build_rasterizer(cfg, dm)\n",
        "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
        "scene_idx = 34\n",
        "indexes = dataset.get_scene_indices(scene_idx)\n",
        "images_satellite = []\n",
        "\n",
        "for idx in indexes:\n",
        "    \n",
        "    data = dataset[idx]\n",
        "    im = data[\"image\"].transpose(1, 2, 0)\n",
        "    im = dataset.rasterizer.to_rgb(im)\n",
        "    # target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
        "    # center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
        "    # draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
        "\n",
        "    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
        "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
        "    draw_trajectory(im, target_positions_pixels, rgb_color=TARGET_POINTS_COLOR,  yaws=data[\"target_yaws\"])\n",
        "    clear_output(wait=True)\n",
        "    images_satellite.append(PIL.Image.fromarray(im[::-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdEi9QImEUI7"
      },
      "outputs": [],
      "source": [
        "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
        "rast = build_rasterizer(cfg, dm)\n",
        "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
        "scene_idx = 34\n",
        "indexes = dataset.get_scene_indices(scene_idx)\n",
        "images_semantic = []\n",
        "\n",
        "for idx in indexes:\n",
        "    \n",
        "    data = dataset[idx]\n",
        "    im = data[\"image\"].transpose(1, 2, 0)\n",
        "    im = dataset.rasterizer.to_rgb(im)\n",
        "    # target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
        "    # center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
        "    # draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
        "\n",
        "    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
        "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
        "    draw_trajectory(im, target_positions_pixels, rgb_color=TARGET_POINTS_COLOR,  yaws=data[\"target_yaws\"])\n",
        "    clear_output(wait=True)\n",
        "    images_semantic.append(PIL.Image.fromarray(im[::-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlwTO07XAZTD"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "def animate_solution(images):\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_data(images[i])\n",
        " \n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(images[0])\n",
        "    \n",
        "    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)\n",
        "\n",
        "anim = animate_solution(images_satellite)\n",
        "HTML(anim.to_jshtml())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ResNet_Lyft_Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}